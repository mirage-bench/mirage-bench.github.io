<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems">
  <meta property="og:title" content="MIRAGE-Bench: Multilingual RAG Evaluation Benchmark"/>
  <meta property="og:description" content="A comprehensive benchmark for evaluating multilingual Retrieval-Augmented Generation (RAG) systems across 10 diverse languages"/>
  <meta property="og:url" content="https://vectara.github.io/mirage-bench/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/mirage_bench_banner.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="MIRAGE-Bench: Multilingual RAG Evaluation">
  <meta name="twitter:description" content="Comprehensive benchmark for evaluating RAG systems across 10 languages to promote multilingual capabilities">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/mirage_bench_banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="RAG, retrieval-augmented generation, multilingual, benchmark, evaluation, NLP, information retrieval, LLM, language models">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MIRAGE-Bench: Multilingual RAG Evaluation Benchmark</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented Generation Systems</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://thakur-nandan.github.io/" target="_blank">Nandan Thakur</a><sup>1</sup></span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/sulemanbkazi/" target="_blank">Suleman Kazi</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.linkedin.com/in/ge-luo-145643244/" target="_blank">Ge Luo</a><sup>2</sup>,</span>
                    <span class="author-block">
                      <a href="https://cs.uwaterloo.ca/~jimmylin/" target="_blank">Jimmy Lin</a><sup>1</sup>,</span>
                      <span class="author-block">
                        <a href="https://www.linkedin.com/in/aminahmad/" target="_blank">Amin Ahmad</a><sup>2</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>University of Waterloo  &nbsp;&nbsp;&nbsp;&nbsp; <sup>2</sup>Vectara<br>NAACL 2025 Main Proceedings</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2410.13716.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/vectara/mirage-bench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.13716" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

            <span class="link-block">
              <a href="https://huggingface.co/collections/nthakur/mirage-bench-naacl25-67ddb6166a7938a37436a455" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fas fa-database"></i>
              </span>
              <span>HF Dataset</span>
              </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <image src="static/images/mirage_bench_banner.png" alt="MIRAGE-Bench Evaluation Framework"/>
      <h2 class="subtitle has-text-centered">
        AI-generated image using Google Gemini showing the mirage effect.
        <!-- MIRAGE-Bench evaluates multilingual Retrieval-Augmented Generation systems across 10 diverse languages, providing a comprehensive assessment framework for both retrieval and generation components. -->
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Retrieval-Augmented Generation (RAG) utilizes Large Language Models (LLMs) to generate factual and correct answers to user questions by citing information available in ground-truth documents. 
            However, at present it's a challenge to evaluate Retrieval-Augmented Generation (RAG) systems both <strong>reliably</strong> and <strong>efficiently</strong> across multiple langauges. Traditional RAG benchmarks can be categorized into two categories:
            <ul>
              <li> <strong>Heuristic-focused:</strong> Manual and traditional approach towards benchmarking. One requires multiple hand-crafted evaluation metrics which require extensive human intervention (in terms of research and experimentation) and/or a gold truth answer which is scarcely available in RAG scenarios.</li>
              <li> <strong>Arena-focused:</strong> An automatic approach where two responses are compared head one in a battle with LLM as a judge (such as MT-Bench). However, automatic evaluations don't necessarily mean they are cheap. LLM-as-a-Judge pairwise comparisons by a high-performing LLM (like GPT-4o) can often shoot up your costs and expenses.</li> 
            </ul>           
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper Introduction -->
<section class="section hero is-dark">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">What is MIRAGE-Bench?</h2>
        <div class="content has-text-justified">
          <p>
            MIRAGE-Bench is a new benchmark designed to improve the evaluation of RAG systems involving human-generated questions in multilingual contexts, e.g., a user asking a question in Hindi and get's the answer back in Hindi. An example is shown in above. MIRAGE-Bench was constructed atop of MIRACL, a multilingual retrieval dataset, and is used to evaluate the generation task in multilingual RAG.
          </p>
          <p>
            MIRAGE-Bench contains a rich diversity of topics by including questions from 18 different languages, cost-effective as it only involves a regression model to compute heuristic-based features and extensible as one can retrain the regression model under a minute with the newer features or additional training data.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
      <h2 class="title is-3"> How Does MIRAGE-Bench Work?</h2>
      <video poster="" id="video1" autoplay controls muted loop height="100%">
        <!-- Your video file here -->
        <source src="static/videos/mirage_bench_teaser.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
       Video shows the step-by-step procedure followed in the construction of MIRAGE-Bench.
      </h2>
      <div class="content has-text-justified">
        <p>
          MIRAGE-Bench is a benchmark which integrates heuristic-based features to learn a regression model for synthetic arena-based evaluations. Below is a step-by-step breakdown of its process:
        </p>
        <ul>
          <li> <strong>Heuristic-Based Features:</strong> MIRAGE assesses 21 different SoTA multilingual-focused LLMs using seven heuristic features. These include metrics such as language detection, citation quality, answer overlap, and fluency. By leveraging these features, MIRAGE-Bench can evaluate RAG systems without requiring human intervention for every evaluation pair.</li>
          <li> <strong>Pairwise Comparisons with GPT-4o:</strong> MIRAGE-Bench incorporates LLM-as-a-Judge pairwise comparisons on a smaller subset of evaluation queries. This step provides high-quality labeled data for training the regression model, ensuring that the trained model is accurate and reliable.</li>
          <li> <strong>Surrogate Judge (Regression Model):</strong> A regression model, such as Random Forest, is trained on the heuristic features and LLM-as-a-Judge results using a Bradley-Terry model and bootstrapping to maximize the juice from a small subset of training points. This model then generates a synthetic arena-based leaderboard, which can cost-effectively evaluate multiple models, by using only heuristic features as input.</li>
      </div>
    </div>
</section>
<!-- End teaser image -->

<!-- Image carousel -->
<section class="section hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/mirage_bench_diagram.png" alt="MIRAGE Flowchart"/>
        <h2 class="subtitle has-text-centered">
          Figure 1: An example of a Multilingual RAG in the Hindi language. The question asks "how many letters are there in binary code?" and the correct answer generated by the multilingual LLM is "binary code contains two letters."
        </h2>
      </div>ß
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/heuristic_feature_scores.png" alt="Heuristic feature scores"/>
        <h2 class="subtitle has-text-centered">
          Figure 2: Lollipop plots denoting the average heuristic-based feature scores achieved by LLMs in MIRAGE-Bench. Each LLM is grouped within the family of LLMs (e.g., Meta).
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/arena_based_leaderboard_scores.png" alt="LLM as a Judge and Heuristic-based Leaderboard Scores"/>
      <h2 class="subtitle has-text-centered">
        Figure 3: Heatmap showing the rank achieved by LLMs on MIRAGE-Bench using LLM-as-a-Judge pairwise comparisons (left) and using surrogate judge scores (right). Kendall tau correlation = 0.909. 
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Key Features Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Observations in MIRAGE-Bench
        </h2>
        <div class="content has-text-justified">
          <p>
          <strong>Quantitative Analysis:</strong> Our learned synthetic leaderboard observes a high correlation (Kendall Tau ($\tau$) = 0.909) with GPT-4o based pairwise comparison results, indicating the effectiveness of the learned regression model.
          </p>
          <p>
            <strong>Heatmaps:</strong> We observed that closed-sourced and large multilingual-focused LLMs dominate multilingual generation, GPT-4 and GPT-4o perform the best with next best being the LLAMA-3-70B model.
          </p>
          <p>
            <strong>Benchmarking:</strong> MIRAGE-Bench benchmarks 21 multilingual-focused LLMs, evaluating their performance across 18 languages. The framework demonstrates a high correlation (Kendall Tau ($\tau$) = 0.909) with GPT-4-based pairwise comparison results, validating its effectiveness.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Key Features Section -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{thakur-mirage:2024,
        author       = {Nandan Thakur and
                        Suleman Kazi and
                        Ge Luo and
                        Jimmy Lin and
                        Amin Ahmad},
        title        = {MIRAGE-Bench: Automatic Multilingual Benchmark Arena for Retrieval-Augmented
                        Generation Systems},
        journal      = {CoRR},
        volume       = {abs/2410.13716},
        year         = {2024},
        url          = {https://doi.org/10.48550/arXiv.2410.13716},
        doi          = {10.48550/ARXIV.2410.13716},
        eprinttype    = {arXiv},
        eprint       = {2410.13716},
        timestamp    = {Wed, 27 Nov 2024 09:01:16 +0100},
        biburl       = {https://dblp.org/rec/journals/corr/abs-2410-13716.bib},
        bibsource    = {dblp computer science bibliography, https://dblp.org}
      }
    </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
